{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d538bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.svm import SVC\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f13d481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_angles = pickle.load(open(\"features_angles.pkl\", \"rb\"))\n",
    "test_fv = pickle.load(open(\"test_fv.pkl\", \"rb\"))\n",
    "train_labels = pickle.load(open(\"train_labels.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0272f833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b4c5557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_angles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5fe1bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 7., 7., 7.,\n",
       "       7., 7., 7., 7., 7., 7., 7., 8., 8., 8., 8., 8., 8., 8., 8., 8., 9.,\n",
       "       9., 9., 9., 9., 9., 9., 9.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "718c1797",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fv = [vector.reshape(10,29,3) for vector in test_fv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22fc547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fv = np.array(test_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f893615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 29, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_fv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36206255",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_angles = [vector.reshape(10,29,3) for vector in features_angles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e90e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_features_angles = np.array(new_features_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff98fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "837c50bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(features_angles,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e39b96ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(test_fv)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c75da1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = features_angles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30d9dc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03148783,  0.05435647, -0.03103553, -0.01694534, -0.02455837,\n",
       "       -0.0289781 , -0.02984959, -0.02570659, -0.0185977 , -0.02783579,\n",
       "       -0.03315584, -0.03608555, -0.03319766, -0.02584614,  0.07335605,\n",
       "       -0.03576313, -0.02920351,  0.0480683 , -0.0185548 , -0.02448564,\n",
       "       -0.00727916,  0.08347139,  0.02965541, -0.03041024, -0.03216949,\n",
       "        0.01411849, -0.03568234,  0.06043157,  0.02990102,  0.03728727,\n",
       "        0.00153906, -0.00397001,  0.00338048, -0.01061022, -0.01080408,\n",
       "       -0.01080408, -0.00514726, -0.0082804 ,  0.0041452 , -0.01220798,\n",
       "       -0.00639831,  0.00639671,  0.00216365,  0.00962088,  0.        ,\n",
       "       -0.01209616,  0.0197729 ,  0.0328069 ,  0.02736929,  0.02736929,\n",
       "        0.060836  ,  0.        ,  0.05783522,  0.03793814, -0.0384714 ,\n",
       "        0.06451139, -0.01940754,  0.05368733, -0.00167791,  0.00757988,\n",
       "       -0.00680986,  0.00680989, -0.00732926, -0.00732926, -0.0019771 ,\n",
       "        0.00529013, -0.00672785,  0.00945391, -0.00964202, -0.01330846,\n",
       "        0.00500723, -0.00619899,  0.01685987,  0.02437703,  0.03174513,\n",
       "       -0.01887301, -0.04368977, -0.04368977, -0.01136789, -0.0167994 ,\n",
       "        0.0324962 , -0.01672618, -0.01946703,  0.0247541 ,  0.03048061,\n",
       "        0.0238467 ,  0.00799749,  0.00685119,  0.00885628,  0.00562464,\n",
       "        0.01015598,  0.01015598, -0.0035598 , -0.00815085,  0.        ,\n",
       "        0.00553786,  0.00611659,  0.00549446,  0.00493167,  0.00958469,\n",
       "        0.00634326, -0.00581775,  0.00771105, -0.00477777,  0.00183116,\n",
       "        0.00183116,  0.01039409,  0.0068181 ,  0.00984438,  0.00669263,\n",
       "        0.00529995,  0.01150443,  0.00485516,  0.01346963,  0.00670619,\n",
       "        0.        ,  0.        ,  0.00635235,  0.00752279,  0.00752279,\n",
       "        0.00677219,  0.00796288, -0.00287748,  0.        ,  0.        ,\n",
       "        0.00877247,  0.        ,  0.0082164 , -0.00145011, -0.00406791,\n",
       "       -0.00211599,  0.00269135, -0.00888141, -0.00888141,  0.        ,\n",
       "        0.01063478,  0.00943275, -0.00766987, -0.00617689,  0.00429536,\n",
       "        0.00618698, -0.01160165,  0.02920875,  0.04554258,  0.02370613,\n",
       "        0.04704865, -0.02076759, -0.02076759,  0.04141133, -0.03501774,\n",
       "        0.04131218, -0.03728646, -0.01274294,  0.03917382,  0.05109724,\n",
       "        0.03684571,  0.01320386, -0.01501125,  0.        ,  0.        ,\n",
       "        0.02161851,  0.02161851,  0.03049875, -0.00415127,  0.03158393,\n",
       "        0.01000707,  0.        ,  0.02585846,  0.        ,  0.02872676,\n",
       "       -0.00677681,  0.00676114,  0.00350965, -0.00284121, -0.00352282,\n",
       "       -0.00352282, -0.00905599,  0.00494196, -0.0108028 ,  0.01252804,\n",
       "       -0.01070944, -0.00408514, -0.00629615,  0.00714386,  0.        ,\n",
       "        0.        ,  0.00696018, -0.00357645,  0.00863045,  0.00863045,\n",
       "        0.00877481,  0.        ,  0.00832827,  0.        ,  0.00666461,\n",
       "        0.0085767 , -0.00553786,  0.01226178, -0.00822008, -0.00911695,\n",
       "       -0.01122438,  0.00680667, -0.00203343, -0.00203343,  0.00935862,\n",
       "        0.00875561, -0.0088887 , -0.00417162,  0.00936193,  0.00840418,\n",
       "       -0.0045369 ,  0.01206729, -0.00934594,  0.00538978, -0.00748461,\n",
       "        0.00686631,  0.00529535,  0.00529535, -0.00130483,  0.00811884,\n",
       "        0.        , -0.00745962,  0.00550243,  0.01092533,  0.00532756,\n",
       "        0.01208887,  0.02177343,  0.07270409,  0.06363488,  0.0686753 ,\n",
       "       -0.07414707, -0.07414707, -0.07961297, -0.05116712, -0.08799997,\n",
       "       -0.081658  ,  0.04887618, -0.05603228,  0.06817417,  0.03076523,\n",
       "        0.00667133,  0.        ,  0.00988974, -0.00257966, -0.00757589,\n",
       "       -0.00757589,  0.0084677 , -0.00896557,  0.00960125,  0.        ,\n",
       "        0.00914194,  0.01085048, -0.00214991,  0.01198992, -0.00212378,\n",
       "       -0.0089814 ,  0.        , -0.01008515,  0.00980268,  0.00980268,\n",
       "        0.00881631, -0.00938542,  0.        , -0.00708129,  0.        ,\n",
       "       -0.004509  , -0.00839283,  0.009992  , -0.02079982, -0.03057245,\n",
       "        0.00993907,  0.        ,  0.05137979,  0.05137979,  0.05893198,\n",
       "       -0.07225792,  0.07610077,  0.03183316,  0.06797261,  0.05816203,\n",
       "        0.        ,  0.06004888,  0.02097799, -0.02099011, -0.02154558,\n",
       "        0.02154559,  0.03770992,  0.03770992,  0.026355  , -0.04988417,\n",
       "       -0.00794425, -0.02355427,  0.03786816,  0.03213349, -0.01199767,\n",
       "        0.03046089, -0.00461065,  0.00709994, -0.0042006 ,  0.00954642,\n",
       "        0.00641597,  0.00641597,  0.0064823 , -0.00707385,  0.00421585,\n",
       "       -0.00600906,  0.00676074,  0.00719526, -0.00328412,  0.00254313,\n",
       "        0.01842275, -0.03309048,  0.07894908,  0.03656662,  0.02118651,\n",
       "        0.02118651,  0.04528184, -0.04680328,  0.05420666, -0.02409581,\n",
       "        0.07792763,  0.04371372, -0.02608343,  0.06179376,  0.04222017,\n",
       "       -0.02913883,  0.        , -0.04452424,  0.0826372 ,  0.0826372 ,\n",
       "        0.09096771, -0.08845259,  0.09506626, -0.05903867,  0.        ,\n",
       "        0.08656643, -0.06453539,  0.09998558, -0.0509656 ,  0.0178692 ,\n",
       "        0.04768531,  0.        , -0.00405861, -0.00405861,  0.05004257,\n",
       "       -0.0256552 ,  0.04965716,  0.0045642 ,  0.03129771,  0.04836517,\n",
       "        0.        ,  0.05667489, -0.00290472,  0.        , -0.00718684,\n",
       "       -0.00213244,  0.00271816,  0.00271816,  0.0065651 ,  0.0068356 ,\n",
       "        0.01212212,  0.        , -0.00773483,  0.00962451,  0.00621979,\n",
       "        0.0129378 ,  0.00502795, -0.00586819,  0.00118421,  0.0052331 ,\n",
       "        0.00829579,  0.00829579,  0.01306698, -0.00792703,  0.0086864 ,\n",
       "        0.01039143, -0.01445473, -0.01092871, -0.01334713,  0.0058866 ,\n",
       "        0.        , -0.03312985,  0.02097116, -0.04752052, -0.03997812,\n",
       "       -0.03997812,  0.02656754,  0.        ,  0.05085652,  0.03919154,\n",
       "       -0.02889145,  0.04278615, -0.03972238,  0.03701418, -0.00284873,\n",
       "        0.00303297,  0.00404181, -0.00713824,  0.00803299,  0.00803299,\n",
       "       -0.00069834, -0.00530251,  0.00632147,  0.00540547,  0.00253519,\n",
       "       -0.0081228 ,  0.00646781,  0.00542716,  0.0525977 ,  0.        ,\n",
       "        0.03438773,  0.        ,  0.05884942,  0.05884942,  0.0706528 ,\n",
       "        0.05825911,  0.07206632,  0.        ,  0.04713036,  0.077501  ,\n",
       "        0.        ,  0.08264685, -0.09994756,  0.0999495 ,  0.09994914,\n",
       "       -0.09994751,  0.        ,  0.        , -0.09994835,  0.09993581,\n",
       "       -0.09994059, -0.09995648, -0.09994731, -0.09994873,  0.09994873,\n",
       "       -0.09994858, -0.02289742,  0.02750812,  0.03094007, -0.043117  ,\n",
       "       -0.0404137 , -0.0404137 ,  0.00997835, -0.02420296,  0.03757666,\n",
       "       -0.03719041, -0.01793534, -0.01259348,  0.00749351,  0.04894378,\n",
       "       -0.00454777, -0.0089978 ,  0.00319172,  0.00890714, -0.01244432,\n",
       "       -0.01244432, -0.00744609, -0.00538447,  0.01017126,  0.00654628,\n",
       "        0.00527358,  0.00469745,  0.00625923,  0.00549901,  0.05435647,\n",
       "        0.0509851 , -0.06580666, -0.07336915,  0.03876407,  0.03876407,\n",
       "       -0.02512966,  0.05435647,  0.02604555,  0.02062701, -0.06457566,\n",
       "        0.02842197, -0.07698797,  0.03222221, -0.00359515, -0.00170588,\n",
       "       -0.0062396 , -0.00623949, -0.00884906, -0.00884906, -0.00317052,\n",
       "       -0.00443444, -0.0047577 ,  0.01096387, -0.00548341, -0.00609166,\n",
       "       -0.00484193, -0.00922114,  0.00884073,  0.02644064,  0.00307271,\n",
       "        0.02432189, -0.04219339, -0.04219339,  0.02508753,  0.02287084,\n",
       "        0.01777742,  0.04319107,  0.03325342,  0.02648223,  0.00985638,\n",
       "        0.01816565,  0.00157641,  0.01032032, -0.00757032, -0.00395363,\n",
       "        0.00624593,  0.00624593, -0.00185471,  0.00425145, -0.02455837,\n",
       "       -0.00514913, -0.00683655, -0.00540356, -0.00642348,  0.01027337,\n",
       "       -0.00892313,  0.0065697 ,  0.00367303, -0.00647964, -0.00625192,\n",
       "       -0.00625192, -0.0289781 , -0.00575072,  0.00464578, -0.0063824 ,\n",
       "        0.00445764,  0.00637268,  0.00733823,  0.00410989,  0.00476081,\n",
       "       -0.02984959, -0.02984959, -0.00586535, -0.00329872, -0.00329872,\n",
       "       -0.00491429,  0.0013325 , -0.00560196, -0.02984959, -0.02984959,\n",
       "       -0.00360498, -0.02984959,  0.0045869 , -0.00460898, -0.00110011,\n",
       "        0.00333638, -0.0061836 , -0.0062564 , -0.0062564 , -0.02570659,\n",
       "       -0.02570659,  0.00799712, -0.00267659,  0.00774151,  0.00662901,\n",
       "       -0.00580273, -0.0094734 ,  0.01887195, -0.04015756, -0.02920239,\n",
       "       -0.04985406,  0.00912351,  0.00912351, -0.03095827,  0.02548335,\n",
       "       -0.02714794,  0.01201321,  0.02112871, -0.01819163, -0.05716028,\n",
       "        0.0084056 ,  0.02632455,  0.01906461, -0.02783579, -0.02783579,\n",
       "        0.01175763,  0.01175763, -0.01881156,  0.03157777, -0.01027032,\n",
       "        0.02824628, -0.02783579,  0.00871688, -0.02783579,  0.01360592,\n",
       "       -0.00784421,  0.00189172,  0.00728235, -0.00609518, -0.00743196,\n",
       "       -0.00743196, -0.0120798 , -0.0136352 , -0.01171602, -0.01496133,\n",
       "        0.00662556,  0.00485929,  0.00289063,  0.00751304, -0.03608555,\n",
       "       -0.03608555, -0.00625459,  0.0040386 ,  0.007269  ,  0.007269  ,\n",
       "       -0.00593901, -0.03608555, -0.00844543, -0.03608555,  0.00252946,\n",
       "        0.00436788,  0.00528662,  0.00497795, -0.00642865,  0.01003084,\n",
       "        0.0081974 ,  0.00746008, -0.00936819, -0.00936819,  0.01025644,\n",
       "       -0.01020657,  0.00867835,  0.0059651 ,  0.00638449, -0.00836817,\n",
       "        0.00570767,  0.00942118,  0.00716427, -0.00328289,  0.0070636 ,\n",
       "       -0.00632733, -0.0125323 , -0.0125323 , -0.01019034,  0.00699799,\n",
       "       -0.02584614, -0.02584614,  0.00793129, -0.00556541, -0.00629223,\n",
       "        0.00174405,  0.04026234, -0.02748434,  0.0804528 ,  0.03796779,\n",
       "       -0.02875959, -0.02875959,  0.01420438,  0.04229954, -0.08314748,\n",
       "       -0.04452428,  0.04559604,  0.04002936, -0.10566317,  0.069348  ,\n",
       "       -0.00736147, -0.03576313,  0.00634709, -0.00195444, -0.00758976,\n",
       "       -0.00758976,  0.0017846 , -0.00552069, -0.00426218, -0.03576313,\n",
       "        0.00625984,  0.00392042,  0.00558239,  0.00732663, -0.00306802,\n",
       "       -0.00175743, -0.02920351, -0.00812349,  0.00601044,  0.00601044,\n",
       "        0.00819973, -0.00309425, -0.02920351,  0.00658635, -0.02920351,\n",
       "       -0.00734468, -0.0073469 ,  0.00963435,  0.04986059,  0.05281137,\n",
       "        0.04649603,  0.0480683 ,  0.04672353,  0.04672353,  0.00673256,\n",
       "       -0.02860996,  0.00875436,  0.06639343,  0.03138555,  0.01639277,\n",
       "        0.0480683 ,  0.02439457,  0.03548064,  0.03552238,  0.03464352,\n",
       "        0.03464373, -0.03643089, -0.03643089,  0.0225214 , -0.0452252 ,\n",
       "        0.03148596,  0.02056555, -0.01968848,  0.01821317,  0.03212609,\n",
       "        0.02043084,  0.00721742, -0.01080592, -0.00844842,  0.00524748,\n",
       "        0.00766121,  0.00766121, -0.00949804, -0.00875874, -0.00447708,\n",
       "       -0.00626243,  0.00236664,  0.00980033,  0.00815038,  0.0062978 ,\n",
       "        0.04061193,  0.01247997, -0.09521881,  0.02783426,  0.04612187,\n",
       "        0.04612187, -0.03085061, -0.00979492, -0.03445296,  0.04046574,\n",
       "       -0.09226341, -0.03299816,  0.04350567, -0.02768807,  0.05952932,\n",
       "        0.07728076,  0.08347139,  0.08308524,  0.04908552,  0.04908552,\n",
       "        0.01044198,  0.05159157, -0.03344116,  0.08110041,  0.08347139,\n",
       "        0.03488273,  0.08355911,  0.0311982 , -0.05502095,  0.03797716,\n",
       "        0.0261415 ,  0.02965541,  0.03605074,  0.03605074, -0.03797691,\n",
       "       -0.01131929,  0.01539343,  0.04872576,  0.04194111,  0.01472069,\n",
       "        0.02965541, -0.01177986,  0.00244045, -0.03041024,  0.00683656,\n",
       "       -0.009391  , -0.0039956 , -0.0039956 , -0.01071722,  0.00692808,\n",
       "        0.00786129, -0.03041024,  0.00701558, -0.00895896,  0.00702838,\n",
       "        0.00293271, -0.00971629, -0.00364794,  0.00444777,  0.00085466,\n",
       "       -0.00286953, -0.00286953, -0.00255715, -0.01343935,  0.00384691,\n",
       "       -0.01157298,  0.00727476, -0.01202034,  0.00747987, -0.00497858,\n",
       "        0.01411849,  0.04246766,  0.04031789, -0.01899965, -0.01650087,\n",
       "       -0.01650087,  0.01722186,  0.01411849, -0.03590207, -0.00258785,\n",
       "        0.01678295, -0.00274248, -0.0388814 ,  0.01885156, -0.00483269,\n",
       "        0.00615567,  0.00435583,  0.00894701,  0.0036411 ,  0.0036411 ,\n",
       "       -0.01161783, -0.00931478, -0.00501527,  0.0073543 ,  0.00575235,\n",
       "       -0.00996375,  0.00599275,  0.00582955,  0.066005  ,  0.06043157,\n",
       "        0.0690073 ,  0.06043157,  0.04468893,  0.04468893, -0.05397746,\n",
       "        0.07970397,  0.01230296,  0.06043157,  0.06683828, -0.01709109,\n",
       "        0.06043157, -0.01726503, -0.09994138, -0.09994838, -0.09994906,\n",
       "       -0.09994038,  0.02990102,  0.02990102, -0.09994704, -0.09989995,\n",
       "        0.09998135, -0.09992926, -0.09994336, -0.0999461 , -0.09994776,\n",
       "       -0.09994262, -0.10184877, -0.06115748, -0.02454961, -0.01490847,\n",
       "        0.02878218,  0.02878218,  0.0340905 , -0.0859927 ,  0.02795698,\n",
       "        0.02415136,  0.01255   , -0.0324526 ,  0.04360857,  0.02486416])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99aa9753",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(10,29,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "549107ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 29, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0094bce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfc291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e73eae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512e0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e5d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super(Patches, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99dcf429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccb1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "#         layers.Normalization(),\n",
    "#         layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(features_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03cae59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2296bda8588>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309414af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = layers.Input(shape=(870,))\n",
    "    # Augment data.\n",
    "#     augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(inputs)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c01233c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"./tmp/checkpoint\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=new_features_angles,\n",
    "        y=train_labels,\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46eb8af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    <ipython-input-7-68cea0bafeb2>:8 call  *\n        patches = tf.image.extract_patches(\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py:6249 extract_image_patches_v2\n        padding, name)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:2388 extract_image_patches\n        rates=rates, padding=padding, name=name)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py:601 _create_op_internal\n        compute_device)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:3569 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:2042 __init__\n        control_input_ops, op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Shape must be rank 4 but is rank 2 for '{{node patches/ExtractImagePatches}} = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 6, 6, 1], padding=\"VALID\", rates=[1, 1, 1, 1], strides=[1, 6, 6, 1]](Placeholder)' with input shapes: [?,870].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e652355ad623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvit_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_vit_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-16e67acf9eb0>\u001b[0m in \u001b[0;36mcreate_vit_classifier\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#     augmented = data_augmentation(inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Create patches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mpatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPatches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Encode patches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mencoded_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPatchEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[1;32m--> 977\u001b[1;33m                                                 input_list)\n\u001b[0m\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m     \u001b[1;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[1;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[0;32m   1113\u001b[0m       \u001b[1;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[1;32m-> 1115\u001b[1;33m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[0;32m   1116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[1;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    847\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 848\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[1;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[0;32m    886\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 888\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 695\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    696\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-7-68cea0bafeb2>:8 call  *\n        patches = tf.image.extract_patches(\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper  **\n        return target(*args, **kwargs)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py:6249 extract_image_patches_v2\n        padding, name)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:2388 extract_image_patches\n        rates=rates, padding=padding, name=name)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:750 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\func_graph.py:601 _create_op_internal\n        compute_device)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:3569 _create_op_internal\n        op_def=op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:2042 __init__\n        control_input_ops, op_def)\n    C:\\Users\\bruno belluzzo\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\framework\\ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Shape must be rank 4 but is rank 2 for '{{node patches/ExtractImagePatches}} = ExtractImagePatches[T=DT_FLOAT, ksizes=[1, 6, 6, 1], padding=\"VALID\", rates=[1, 1, 1, 1], strides=[1, 6, 6, 1]](Placeholder)' with input shapes: [?,870].\n"
     ]
    }
   ],
   "source": [
    "vit_classifier = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "001f8ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 13s 13s/step - loss: 5.0838 - accuracy: 0.0732 - top-5-accuracy: 0.4512 - val_loss: 14.0781 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.8542 - accuracy: 0.1585 - top-5-accuracy: 0.7317 - val_loss: 14.6006 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.5678 - accuracy: 0.2317 - top-5-accuracy: 0.7195 - val_loss: 15.7181 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 3.2761 - accuracy: 0.2073 - top-5-accuracy: 0.8293 - val_loss: 16.5715 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.7188 - accuracy: 0.3171 - top-5-accuracy: 0.8171 - val_loss: 14.2665 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1186 - accuracy: 0.4512 - top-5-accuracy: 0.9268 - val_loss: 15.3444 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.5671 - accuracy: 0.3293 - top-5-accuracy: 0.9146 - val_loss: 19.1570 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.1102 - accuracy: 0.4390 - top-5-accuracy: 0.9756 - val_loss: 19.3583 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.9197 - accuracy: 0.4390 - top-5-accuracy: 0.9634 - val_loss: 21.3195 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 2.3843 - accuracy: 0.4756 - top-5-accuracy: 0.9512 - val_loss: 22.1350 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.7899 - accuracy: 0.5854 - top-5-accuracy: 1.0000 - val_loss: 20.6800 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5584 - accuracy: 0.5488 - top-5-accuracy: 0.9878 - val_loss: 19.8683 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.4265 - accuracy: 0.6585 - top-5-accuracy: 0.9512 - val_loss: 20.1323 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.7027 - accuracy: 0.5244 - top-5-accuracy: 1.0000 - val_loss: 21.7054 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3370 - accuracy: 0.6585 - top-5-accuracy: 0.9878 - val_loss: 23.2667 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.2995 - accuracy: 0.7073 - top-5-accuracy: 0.9634 - val_loss: 22.8584 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0555 - accuracy: 0.6707 - top-5-accuracy: 0.9756 - val_loss: 23.2419 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.5043 - accuracy: 0.6951 - top-5-accuracy: 0.9512 - val_loss: 24.0791 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.3683 - accuracy: 0.6951 - top-5-accuracy: 0.9756 - val_loss: 26.3762 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7207 - accuracy: 0.7805 - top-5-accuracy: 1.0000 - val_loss: 28.3714 - val_accuracy: 0.0000e+00 - val_top-5-accuracy: 0.2000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0474 - accuracy: 0.7439 - top-5-accuracy: 1.0000 - val_loss: 25.3884 - val_accuracy: 0.1000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9104 - accuracy: 0.7683 - top-5-accuracy: 1.0000 - val_loss: 21.7731 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0765 - accuracy: 0.7561 - top-5-accuracy: 1.0000 - val_loss: 19.8614 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7302 - accuracy: 0.7927 - top-5-accuracy: 1.0000 - val_loss: 19.2132 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7648 - accuracy: 0.7805 - top-5-accuracy: 1.0000 - val_loss: 19.5620 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7488 - accuracy: 0.7683 - top-5-accuracy: 1.0000 - val_loss: 19.1797 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6149 - accuracy: 0.8049 - top-5-accuracy: 1.0000 - val_loss: 18.9965 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.0369 - accuracy: 0.7195 - top-5-accuracy: 1.0000 - val_loss: 18.5557 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7476 - accuracy: 0.8049 - top-5-accuracy: 0.9878 - val_loss: 18.7603 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8224 - accuracy: 0.7317 - top-5-accuracy: 0.9878 - val_loss: 18.5703 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8462 - accuracy: 0.8049 - top-5-accuracy: 1.0000 - val_loss: 18.8345 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.7459 - accuracy: 0.7561 - top-5-accuracy: 0.9878 - val_loss: 18.7771 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8501 - accuracy: 0.7561 - top-5-accuracy: 1.0000 - val_loss: 18.9993 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3167 - accuracy: 0.8780 - top-5-accuracy: 1.0000 - val_loss: 19.2456 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2870 - accuracy: 0.8659 - top-5-accuracy: 1.0000 - val_loss: 19.7647 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.8116 - accuracy: 0.8049 - top-5-accuracy: 1.0000 - val_loss: 19.6407 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3538 - accuracy: 0.9268 - top-5-accuracy: 1.0000 - val_loss: 19.3777 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6034 - accuracy: 0.8049 - top-5-accuracy: 1.0000 - val_loss: 19.3302 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4086 - accuracy: 0.8902 - top-5-accuracy: 1.0000 - val_loss: 18.9595 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.6298 - accuracy: 0.8659 - top-5-accuracy: 1.0000 - val_loss: 18.8005 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3522 - accuracy: 0.9024 - top-5-accuracy: 1.0000 - val_loss: 18.8900 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3760 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 18.9129 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5623 - accuracy: 0.8659 - top-5-accuracy: 0.9878 - val_loss: 18.5419 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2703 - accuracy: 0.8902 - top-5-accuracy: 0.9878 - val_loss: 18.4590 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.5746 - accuracy: 0.8537 - top-5-accuracy: 1.0000 - val_loss: 18.1653 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3477 - accuracy: 0.8780 - top-5-accuracy: 1.0000 - val_loss: 18.2770 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4441 - accuracy: 0.8293 - top-5-accuracy: 1.0000 - val_loss: 18.7237 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2661 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 19.5881 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1434 - accuracy: 0.9634 - top-5-accuracy: 1.0000 - val_loss: 20.8874 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3930 - accuracy: 0.8659 - top-5-accuracy: 1.0000 - val_loss: 22.0106 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1999 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 22.6827 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2827 - accuracy: 0.9024 - top-5-accuracy: 1.0000 - val_loss: 22.2852 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2193 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 21.3495 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.9506 - accuracy: 0.8659 - top-5-accuracy: 0.9878 - val_loss: 19.2241 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2019 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 17.0434 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2993 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 15.3886 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3354 - accuracy: 0.8902 - top-5-accuracy: 1.0000 - val_loss: 14.6562 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3118 - accuracy: 0.8902 - top-5-accuracy: 1.0000 - val_loss: 15.0449 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1825 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 16.4022 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4524 - accuracy: 0.8659 - top-5-accuracy: 1.0000 - val_loss: 17.5697 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2990 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 17.2004 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4978 - accuracy: 0.8902 - top-5-accuracy: 1.0000 - val_loss: 16.8331 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3585 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 17.4663 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4368 - accuracy: 0.8293 - top-5-accuracy: 1.0000 - val_loss: 18.3746 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2201 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 19.1698 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1630 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 20.1589 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.4768 - accuracy: 0.8780 - top-5-accuracy: 1.0000 - val_loss: 21.0537 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1795 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 21.7113 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2658 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 21.9345 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3178 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 21.6013 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1895 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 21.2112 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3110 - accuracy: 0.8902 - top-5-accuracy: 1.0000 - val_loss: 20.3125 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0881 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 19.7829 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0449 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 19.2446 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2208 - accuracy: 0.9268 - top-5-accuracy: 1.0000 - val_loss: 18.8053 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2625 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 18.7407 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1471 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 19.0307 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2564 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 18.3629 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0255 - accuracy: 0.9878 - top-5-accuracy: 1.0000 - val_loss: 18.1093 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3165 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 17.9871 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3907 - accuracy: 0.9024 - top-5-accuracy: 1.0000 - val_loss: 18.9134 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2162 - accuracy: 0.9634 - top-5-accuracy: 1.0000 - val_loss: 20.4176 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2857 - accuracy: 0.9268 - top-5-accuracy: 1.0000 - val_loss: 21.8302 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1372 - accuracy: 0.9756 - top-5-accuracy: 1.0000 - val_loss: 23.2244 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.0764 - accuracy: 0.9756 - top-5-accuracy: 1.0000 - val_loss: 24.6348 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1674 - accuracy: 0.9634 - top-5-accuracy: 1.0000 - val_loss: 25.1139 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3740 - accuracy: 0.9146 - top-5-accuracy: 1.0000 - val_loss: 23.9773 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2120 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 22.4377 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.3069 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 21.1069 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2732 - accuracy: 0.9634 - top-5-accuracy: 1.0000 - val_loss: 20.3707 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0439 - accuracy: 0.9756 - top-5-accuracy: 1.0000 - val_loss: 20.0466 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2787 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 19.6492 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2147 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 19.0714 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2355 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 18.6752 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2498 - accuracy: 0.9512 - top-5-accuracy: 1.0000 - val_loss: 18.6291 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2177 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 19.7788 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.2015 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 21.0041 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.0922 - accuracy: 0.9390 - top-5-accuracy: 1.0000 - val_loss: 21.3751 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1653 - accuracy: 0.9268 - top-5-accuracy: 1.0000 - val_loss: 21.5165 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 3s 3s/step - loss: 0.1428 - accuracy: 0.9634 - top-5-accuracy: 1.0000 - val_loss: 21.1257 - val_accuracy: 0.2000 - val_top-5-accuracy: 0.2000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-d28eed0889c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvit_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-81-f813c0fab0ea>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_5_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test accuracy: {round(accuracy * 100, 2)}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "history = run_experiment(vit_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0d9205d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x2296bd0d408>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_classifier.load_weights(\"./tmp/checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e02c6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-a3d6b6323348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_features_angles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.evaluate(new_features_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc78d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resposta = vit_classifier.predict(test_fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fd2ec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.3984425 ,   7.184504  ,  -0.04978048,   5.6326504 ,\n",
       "        -3.816053  ,   7.013057  ,  -3.1799543 ,  -5.814621  ,\n",
       "         8.616629  , -18.301085  ], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resposta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f73b9a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(resposta[0].argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f03817",
   "metadata": {},
   "source": [
    "# Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "624bb31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726979f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "253ca7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "17473536/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4018e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f02b9066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  25, 100, ...,  19, 178,  32],\n",
       "       [  0,   0,   0, ...,  16, 145,  95],\n",
       "       [  0,   0,   0, ...,   7, 129, 113],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   4,   2,   2],\n",
       "       [  0,   0,   0, ...,  12,   9,  23],\n",
       "       [  0,   0,   0, ..., 204, 131,   9]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0d3394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e27e92f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 200)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f41579e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 870)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_angles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f8abdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 96  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(870,))\n",
    "embedding_layer = TokenAndPositionEmbedding(870, 2000, 96)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "00fbdaf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,   25,  100,   43,  838,  112,   50,  670,    2,    9,   35,\n",
       "        480,  284,    5,  150,    4,  172,  112,  167,    2,  336,  385,\n",
       "         39,    4,  172,    2, 1111,   17,  546,   38,   13,  447,    4,\n",
       "        192,   50,   16,    6,  147,    2,   19,   14,   22,    4, 1920,\n",
       "          2,  469,    4,   22,   71,   87,   12,   16,   43,  530,   38,\n",
       "         76,   15,   13, 1247,    4,   22,   17,  515,   17,   12,   16,\n",
       "        626,   18,    2,    5,   62,  386,   12,    8,  316,    8,  106,\n",
       "          5,    4,    2,    2,   16,  480,   66,    2,   33,    4,  130,\n",
       "         12,   16,   38,  619,    5,   25,  124,   51,   36,  135,   48,\n",
       "         25, 1415,   33,    6,   22,   12,  215,   28,   77,   52,    5,\n",
       "         14,  407,   16,   82,    2,    8,    4,  107,  117,    2,   15,\n",
       "        256,    4,    2,    7,    2,    5,  723,   36,   71,   43,  530,\n",
       "        476,   26,  400,  317,   46,    7,    4,    2, 1029,   13,  104,\n",
       "         88,    4,  381,   15,  297,   98,   32,    2,   56,   26,  141,\n",
       "          6,  194,    2,   18,    4,  226,   22,   21,  134,  476,   26,\n",
       "        480,    5,  144,   30,    2,   18,   51,   36,   28,  224,   92,\n",
       "         25,  104,    4,  226,   65,   16,   38, 1334,   88,   12,   16,\n",
       "        283,    5,   16,    2,  113,  103,   32,   15,   16,    2,   19,\n",
       "        178,   32])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "584acedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03257373,  0.01453273, -0.03061472, -0.02082982,  0.11790981,\n",
       "       -0.02703652,  0.03362912,  0.03371414, -0.03006495, -0.03442991,\n",
       "        0.0408205 , -0.03565749,  0.03599875,  0.0981624 ,  0.07726734,\n",
       "       -0.0374425 , -0.0321108 , -0.03768962, -0.02173856, -0.03717138,\n",
       "       -0.03183392, -0.02701145, -0.03339936, -0.03402802, -0.02630187,\n",
       "       -0.02221895, -0.03177619,  0.00994954, -0.03073265, -0.02716654,\n",
       "       -0.02109196,  0.02010781,  0.01887883, -0.00706891, -0.02915106,\n",
       "       -0.02915106,  0.02212439, -0.02313733, -0.01751082, -0.02406493,\n",
       "        0.01694044,  0.00758309, -0.02348271,  0.00658279,  0.01951865,\n",
       "        0.04501304,  0.06326226,  0.01805053, -0.02921667, -0.02921667,\n",
       "       -0.00914365, -0.02466349,  0.03104447, -0.0274369 , -0.03935496,\n",
       "        0.01722397,  0.06138108,  0.02177665,  0.00828035, -0.01002643,\n",
       "        0.        , -0.00667753,  0.01079002,  0.01079002,  0.00803378,\n",
       "       -0.00783358,  0.        , -0.00697727,  0.        , -0.00506242,\n",
       "       -0.00625327,  0.0096974 ,  0.02311869, -0.02012265,  0.08017238,\n",
       "        0.04003761,  0.02240258,  0.02240258,  0.04132056, -0.04313682,\n",
       "        0.04639602, -0.0294708 ,  0.07237444,  0.04149407,  0.03609674,\n",
       "        0.04546575,  0.039513  , -0.11955476, -0.07225598,  0.        ,\n",
       "        0.11044346,  0.11044346,  0.12223438, -0.12826568,  0.13213155,\n",
       "       -0.1093829 ,  0.09683435,  0.12470421,  0.        ,  0.11188627,\n",
       "       -0.0107963 ,  0.00549691, -0.00675276,  0.00728289,  0.00602946,\n",
       "        0.00602946, -0.00190319,  0.01012516,  0.        , -0.00713698,\n",
       "        0.00754057,  0.0114684 ,  0.00549606,  0.01272112, -0.03037478,\n",
       "       -0.01799124,  0.03536498, -0.04606455, -0.04861892, -0.04861892,\n",
       "       -0.01190954,  0.01578092,  0.03609937,  0.01042901, -0.02576233,\n",
       "       -0.01164738,  0.03928672,  0.04055157,  0.0354957 ,  0.02199779,\n",
       "        0.        , -0.0301665 ,  0.04276435,  0.04276435,  0.0511374 ,\n",
       "       -0.03125208,  0.05409113,  0.0280329 ,  0.        ,  0.05693687,\n",
       "       -0.03606488,  0.06866153,  0.0017148 ,  0.00292703,  0.00723919,\n",
       "       -0.00645379, -0.00648713, -0.00648713,  0.01182486,  0.01108289,\n",
       "        0.0107144 , -0.00271535, -0.00482734,  0.00903481,  0.00620445,\n",
       "        0.00975013, -0.00674215,  0.00655912,  0.00809941, -0.0080994 ,\n",
       "        0.00108863,  0.00108863, -0.00430139,  0.00224652,  0.0048992 ,\n",
       "        0.01044855, -0.01008805, -0.00828218, -0.0067232 , -0.00532404,\n",
       "        0.        ,  0.03467648,  0.05439657,  0.        , -0.07684446,\n",
       "       -0.07684446,  0.04730944,  0.        ,  0.06007551,  0.06423648,\n",
       "        0.0355865 ,  0.06057902,  0.        ,  0.07116376, -0.0064842 ,\n",
       "        0.00648005, -0.00465373, -0.00863767,  0.00948273,  0.00948273,\n",
       "        0.00998598, -0.0083772 , -0.00598654,  0.01180549, -0.01388843,\n",
       "       -0.00827755, -0.01107388,  0.00881094,  0.        , -0.02455818,\n",
       "        0.03773196,  0.02149437, -0.02420125, -0.02420125,  0.03789716,\n",
       "        0.        ,  0.04805122,  0.05279262, -0.04404244,  0.04672357,\n",
       "        0.05549764,  0.04114681, -0.01331127, -0.04958727,  0.        ,\n",
       "       -0.0632031 ,  0.10476863,  0.10476863,  0.0966445 , -0.12823091,\n",
       "        0.10629166, -0.06960674,  0.        ,  0.0987921 , -0.0795308 ,\n",
       "        0.10036098,  0.01894715,  0.0794123 ,  0.06621472,  0.08032448,\n",
       "       -0.07690851, -0.07690851, -0.08500895, -0.05024214, -0.09623497,\n",
       "       -0.08697216,  0.04447006, -0.06224356,  0.07310651,  0.03061855,\n",
       "        0.00753892,  0.        ,  0.01091966,  0.00263591, -0.00816586,\n",
       "       -0.00816586,  0.00874183, -0.00989514,  0.00986258,  0.        ,\n",
       "        0.00992697,  0.0113949 ,  0.00224152,  0.01265922,  0.00770561,\n",
       "       -0.00961171, -0.00868396,  0.01063441, -0.0058007 , -0.0058007 ,\n",
       "        0.00826305, -0.00608733,  0.00221506,  0.00790281,  0.00651529,\n",
       "        0.00448285,  0.00721252,  0.01100841, -0.00130161,  0.00289268,\n",
       "        0.0062261 , -0.00623968,  0.01116405,  0.01116405,  0.00578502,\n",
       "       -0.00150925,  0.00260733,  0.00419401,  0.00605523, -0.01293143,\n",
       "        0.00749721,  0.00711036, -0.03286525, -0.01882734,  0.03755989,\n",
       "        0.        ,  0.00801639,  0.00801639,  0.02942427,  0.03201848,\n",
       "        0.02559199,  0.02715546,  0.0198802 ,  0.02952118,  0.        ,\n",
       "        0.03471378,  0.        ,  0.        , -0.00617268, -0.00371592,\n",
       "       -0.00716583, -0.00716583,  0.00614272,  0.        ,  0.00769941,\n",
       "        0.        , -0.00652129,  0.00975568, -0.00473181,  0.01321942,\n",
       "       -0.00791436,  0.00690696, -0.00756404, -0.00720073, -0.0047631 ,\n",
       "       -0.0047631 ,  0.01088786,  0.00892462,  0.        ,  0.00635897,\n",
       "        0.00217009,  0.00993669, -0.00719885,  0.01239793, -0.01049509,\n",
       "        0.01318283,  0.        ,  0.        ,  0.0249807 ,  0.0249807 ,\n",
       "        0.03559236, -0.02098831,  0.03405741,  0.01334865,  0.        ,\n",
       "        0.02777224,  0.        ,  0.03165654, -0.00621633,  0.00315089,\n",
       "        0.00705438, -0.00864944,  0.00719614,  0.00719614, -0.00701864,\n",
       "       -0.00881079, -0.00215869, -0.01203552, -0.00444892,  0.00790488,\n",
       "        0.00709997,  0.0077145 , -0.01037895,  0.00552076,  0.00832365,\n",
       "        0.        ,  0.01144367,  0.01144367,  0.01787255, -0.01657824,\n",
       "        0.01899927, -0.0017383 ,  0.01657629,  0.01253944,  0.        ,\n",
       "        0.01798542,  0.00652408,  0.01155507, -0.0099048 ,  0.00990478,\n",
       "       -0.0094039 , -0.0094039 ,  0.00242138, -0.00376332, -0.00914928,\n",
       "        0.00511967, -0.00930778, -0.01323877,  0.00500819, -0.00688053,\n",
       "       -0.01086508,  0.0120775 ,  0.00883893, -0.01051914, -0.01110473,\n",
       "       -0.01110473, -0.00859225, -0.00335133,  0.01125615, -0.01238571,\n",
       "       -0.00879021,  0.01114108, -0.00987482,  0.01043732,  0.00682994,\n",
       "        0.        , -0.00426319,  0.00584061,  0.00442567,  0.00442567,\n",
       "        0.0053482 ,  0.00631246,  0.01177838,  0.        , -0.00430495,\n",
       "        0.00358305,  0.00581357,  0.01182452,  0.        , -0.01550858,\n",
       "        0.        ,  0.01527594,  0.02630253,  0.02630253,  0.04287901,\n",
       "        0.        ,  0.0435503 ,  0.02599278,  0.        ,  0.04830905,\n",
       "       -0.01487355,  0.0444285 ,  0.        ,  0.00106221, -0.00551026,\n",
       "        0.00931331,  0.00538642,  0.00538642,  0.00944474,  0.        ,\n",
       "        0.00562386, -0.00683355,  0.00572169, -0.00566594,  0.00799715,\n",
       "        0.00992911,  0.00923356,  0.00763111,  0.009706  ,  0.0059782 ,\n",
       "        0.00969328,  0.00969328,  0.00545268, -0.00930734,  0.        ,\n",
       "        0.00571519,  0.00798851,  0.00645577,  0.00525467,  0.01056576,\n",
       "        0.01218491,  0.01589183,  0.0184202 , -0.01088999, -0.03512763,\n",
       "       -0.03512763,  0.00544644, -0.03663654,  0.0253999 , -0.03313477,\n",
       "        0.02317716,  0.02900535,  0.00611356,  0.02308593, -0.01990119,\n",
       "        0.01893107, -0.04603257, -0.02941432, -0.03982038, -0.03982038,\n",
       "        0.03500047,  0.01137074,  0.026928  ,  0.03240623,  0.01889   ,\n",
       "        0.03378084, -0.04683073,  0.0310343 ,  0.00836061,  0.00033436,\n",
       "       -0.03061472, -0.0026758 ,  0.00604817,  0.00604817,  0.0063366 ,\n",
       "        0.01052654, -0.03061472,  0.00629816, -0.03061472, -0.00869902,\n",
       "        0.00281718,  0.00981244,  0.0314218 , -0.02534421, -0.10735286,\n",
       "       -0.04344864,  0.03291684,  0.03291684, -0.02616068,  0.01619615,\n",
       "       -0.02576793,  0.03653379, -0.08975298, -0.01725477, -0.0440572 ,\n",
       "        0.00699845,  0.0687273 , -0.06320136,  0.10288926,  0.11790981,\n",
       "        0.09419257,  0.09419257,  0.07549691,  0.02884455, -0.04130721,\n",
       "       -0.05674603,  0.11258931,  0.06858538,  0.11790981,  0.08615757,\n",
       "        0.00696416, -0.00306643,  0.00480456, -0.00654606, -0.0132157 ,\n",
       "       -0.0132157 , -0.01058051,  0.00654545, -0.02703652, -0.02703652,\n",
       "        0.00704378, -0.00581467, -0.00660585,  0.00220068, -0.0835574 ,\n",
       "       -0.06191185, -0.04508444, -0.02529388,  0.02445673,  0.02445673,\n",
       "        0.04148022, -0.0733044 ,  0.02799648,  0.00816638, -0.03289883,\n",
       "       -0.0198078 ,  0.02812974,  0.03314002,  0.02668186,  0.05610547,\n",
       "        0.03371414,  0.04171242,  0.0277833 ,  0.0277833 , -0.01605826,\n",
       "        0.03107922,  0.00738156,  0.06204298,  0.03371414,  0.0162772 ,\n",
       "        0.045968  , -0.03743377, -0.00729899,  0.00808413,  0.00606762,\n",
       "       -0.00514467, -0.00888419, -0.00888419, -0.03006495,  0.00294659,\n",
       "        0.00778209, -0.00604534,  0.00915206,  0.00652111,  0.00663977,\n",
       "       -0.005704  , -0.00638215, -0.00398517, -0.00994146, -0.00994193,\n",
       "       -0.01070872, -0.01070872, -0.01192657,  0.00571773, -0.00552009,\n",
       "        0.01044419,  0.00315306, -0.00548713,  0.00117783, -0.0019877 ,\n",
       "        0.0408205 ,  0.06635344,  0.05132928,  0.0408205 , -0.09694046,\n",
       "       -0.09694046,  0.03991889,  0.0408205 , -0.03661593,  0.03439658,\n",
       "        0.06591888, -0.02095921,  0.0408205 , -0.02576006, -0.00763262,\n",
       "        0.00320962,  0.00936956,  0.00854534, -0.00571593, -0.00571593,\n",
       "       -0.00877046, -0.01328756, -0.01006343, -0.0138607 ,  0.00765326,\n",
       "       -0.00833614,  0.00674378,  0.00774904,  0.03599875,  0.04617045,\n",
       "       -0.0482815 , -0.06382324,  0.03025275,  0.03025275,  0.03454275,\n",
       "        0.03599875,  0.02511917, -0.0261192 , -0.08186314,  0.02878331,\n",
       "       -0.05870737,  0.02703874,  0.06150477,  0.08550177,  0.0981624 ,\n",
       "        0.09105336, -0.05508829, -0.05508829,  0.02880572, -0.08367476,\n",
       "       -0.06366252,  0.08962722,  0.0981624 ,  0.0481955 ,  0.08926286,\n",
       "        0.05352025,  0.04188955, -0.02364928,  0.0877477 ,  0.03304906,\n",
       "       -0.02241854, -0.02241854,  0.01060022,  0.04642708, -0.10336507,\n",
       "       -0.04451175,  0.06035854,  0.03921689, -0.10401501,  0.07182547,\n",
       "       -0.00684798, -0.0374425 ,  0.00818268,  0.00707647, -0.00780917,\n",
       "       -0.00780917, -0.0012388 , -0.00567683, -0.00499308, -0.0374425 ,\n",
       "        0.00660684,  0.00461818,  0.00892802,  0.00747833, -0.00156988,\n",
       "        0.01006452, -0.01003458, -0.0072691 ,  0.00345553,  0.00345553,\n",
       "        0.00696403, -0.0103991 ,  0.01020889,  0.00474235, -0.00534856,\n",
       "       -0.00924583,  0.00593769,  0.01056589, -0.00632412,  0.00726746,\n",
       "        0.00493321,  0.00404928,  0.00873976,  0.00873976, -0.00671362,\n",
       "       -0.01300309, -0.00447987,  0.00520533,  0.0070288 , -0.00934206,\n",
       "        0.00561107,  0.01010142, -0.00886229,  0.02617966, -0.01681014,\n",
       "       -0.02173856,  0.02699259,  0.02699259, -0.01690546, -0.00608573,\n",
       "        0.02360814,  0.03058581,  0.02592569,  0.01302829, -0.02173856,\n",
       "        0.01129586, -0.03717138, -0.03717138, -0.00541542, -0.0070147 ,\n",
       "        0.0074204 ,  0.0074204 , -0.00830474, -0.03717138, -0.00598113,\n",
       "       -0.03717138, -0.00497548,  0.00833233, -0.00752391,  0.0113177 ,\n",
       "        0.00242736,  0.00476466,  0.00823546, -0.00561556, -0.01100435,\n",
       "       -0.01100435,  0.01216577,  0.00308947, -0.03183392,  0.00964602,\n",
       "        0.0101016 , -0.00725087,  0.00768948,  0.00605794,  0.02757117,\n",
       "        0.02391692, -0.02701145, -0.02701145,  0.00740724,  0.00740724,\n",
       "       -0.02121153,  0.02585809, -0.01019617,  0.03564271, -0.02701145,\n",
       "        0.01377751, -0.02701145,  0.01198169,  0.00530398, -0.00823211,\n",
       "       -0.00468824, -0.00328038, -0.0050335 , -0.0050335 , -0.00735911,\n",
       "       -0.00729549,  0.01190049, -0.00660917,  0.00468599,  0.00268269,\n",
       "        0.00714438,  0.00392156,  0.01026893,  0.01263583,  0.00952504,\n",
       "       -0.03402802,  0.0086403 ,  0.0086403 ,  0.00505601,  0.00972525,\n",
       "       -0.01591902,  0.01341217,  0.00487204, -0.00880103, -0.03402802,\n",
       "        0.00808405, -0.00728649,  0.01237866,  0.00309758,  0.00310906,\n",
       "       -0.0078581 , -0.0078581 ,  0.01166166,  0.00360728, -0.00809853,\n",
       "        0.01151556, -0.00496428,  0.00898285, -0.00451716, -0.00916178,\n",
       "        0.01102317,  0.01053758,  0.0071466 ,  0.01111636, -0.00390121,\n",
       "       -0.00390121,  0.0044633 , -0.00991829,  0.00944092,  0.00975092,\n",
       "        0.00482173,  0.00591517,  0.01057328, -0.01177228,  0.00722562,\n",
       "       -0.03177619,  0.00287916, -0.00536789, -0.00802088, -0.00802088,\n",
       "       -0.01103653,  0.00606246, -0.0057578 , -0.03177619,  0.0060712 ,\n",
       "       -0.00971902,  0.00310355, -0.00660799,  0.00994954,  0.03653153,\n",
       "        0.00994954,  0.04131136,  0.02669452,  0.02669452, -0.01117815,\n",
       "        0.00994954,  0.00865131,  0.03864214,  0.00994954, -0.03283888,\n",
       "        0.04297985, -0.01562949, -0.03073265, -0.00463739,  0.00886267,\n",
       "       -0.00878323, -0.01020862, -0.01020862,  0.01013622, -0.03073265,\n",
       "       -0.00779099,  0.00886971,  0.01059935, -0.00882638, -0.00922227,\n",
       "        0.00813118,  0.00062346,  0.00837748, -0.00845943, -0.00469208,\n",
       "        0.00866584,  0.00866584,  0.00657052, -0.00192921, -0.02716654,\n",
       "       -0.00593494, -0.00561324, -0.00621643, -0.00666538,  0.0105139 ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_angles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "646836fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " indices[545] = 545 is not in [0, 200)\n\t [[node model_5/token_and_position_embedding_7/embedding_15/embedding_lookup (defined at <ipython-input-22-7321ccbd6d67>:10) ]] [Op:__inference_train_function_28381]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_5/token_and_position_embedding_7/embedding_15/embedding_lookup:\n model_5/token_and_position_embedding_7/embedding_15/embedding_lookup/27592 (defined at C:\\Anaconda\\envs\\harbop\\lib\\contextlib.py:112)\t\n model_5/token_and_position_embedding_7/range (defined at <ipython-input-22-7321ccbd6d67>:9)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-b3a36c00f6e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m history = model.fit(\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mfeatures_angles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3040\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3042\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1964\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[545] = 545 is not in [0, 200)\n\t [[node model_5/token_and_position_embedding_7/embedding_15/embedding_lookup (defined at <ipython-input-22-7321ccbd6d67>:10) ]] [Op:__inference_train_function_28381]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_5/token_and_position_embedding_7/embedding_15/embedding_lookup:\n model_5/token_and_position_embedding_7/embedding_15/embedding_lookup/27592 (defined at C:\\Anaconda\\envs\\harbop\\lib\\contextlib.py:112)\t\n model_5/token_and_position_embedding_7/range (defined at <ipython-input-22-7321ccbd6d67>:9)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    features_angles, train_labels, batch_size=32, epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50c53158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 3., 3., 3., 3., 3., 3.,\n",
       "       3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 5., 5., 5., 5., 5., 5.,\n",
       "       5., 5., 5., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 7., 7., 7., 7.,\n",
       "       7., 7., 7., 7., 7., 7., 8., 8., 8., 8., 8., 8., 8., 8., 8., 9., 9.,\n",
       "       9., 9., 9., 9., 9., 9., 9.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "75b2b4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(features_angles[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a5c20251",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(np.array(features_angles[0:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "53b77bef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3b1ed1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09963427, 0.10062379, 0.10103338, 0.09928057, 0.10065974,\n",
       "       0.09960502, 0.09900425, 0.09938139, 0.10063899, 0.10013861],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16461ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Harbop",
   "language": "python",
   "name": "harbop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
